% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/core_articles_pdf.R
\name{core_articles_pdf}
\alias{core_articles_pdf}
\alias{core_articles_pdf_}
\title{Download article pdf}
\usage{
core_articles_pdf(id, key = NULL, overwrite = TRUE, parse = TRUE, ...)

core_articles_pdf_(id, key = NULL, overwrite = TRUE, ...)
}
\arguments{
\item{id}{(integer) CORE ID of the article that needs to be fetched.
Required}

\item{key}{A IUCN API token}

\item{overwrite}{(logical) overwrite file or not. Default: \code{TRUE}}

\item{parse}{(logical) Whether to parse to list (`FALSE`) or
data.frame (`TRUE`). Default: `TRUE`}

\item{...}{Curl options passed to [crul::HttpClient()]}
}
\description{
Download article pdf
}
\details{
\code{core_articles_pdf} does the HTTP request and parses
PDF to text, while \code{core_articles_pdf_} just does the HTTP request
and gives back the path to the file

If you get a message like \code{Error: Not Found (HTTP 404)}, that means
a PDF was not found. That is, it does not exist. That is, there is no
PDF associated with the article ID you searched for. This is the
correct behavior, and nothing is wrong with this function or this
package. We could do another web request to check if the id you
pass in has a PDF or not first, but that's another request, slowing
this function down.

These functions take one article ID at a time. Use lapply/loops/etc for
many ids
}
\examples{
\dontrun{
# just http request, get file path back
core_articles_pdf_(11549557)

# get paper and parse to text
core_articles_pdf(11549557)

ids <- c(11549557, 385071)
res <- lapply(ids, core_articles_pdf)
vapply(res, "[[", "", 1)
}
}
\references{
\url{https://core.ac.uk/docs/#!/articles/getArticlePdfByCoreId}
}
